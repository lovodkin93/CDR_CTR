# <h2 align="center"> Don’t Add, don’t Miss: Effective Content Preserving Generation from Pre-Selected Text Spans </h2>

Repository for our EMNLP 2023 findings paper "[Don’t Add, don’t Miss: Effective Content Preserving Generation from Pre-Selected Text Spans](https://aclanthology.org/2023.findings-emnlp.852/)"

In this repository, we include our 3 techniques to improve the Controlled Text Reduction (CTR) task: {C}ontrolled decoding, {D}istillation from GPT-4 and {R}einforcement Learning (CDR).

## Download Dataset
To download the original Controlled Text Reduction dataset, follow the instructions in [link](https://github.com/lovodkin93/Controlled_Text_Reduction), and save it under the `data`. \
For the GPT-4 distilled training data, download it from [GPT4-distilled data](https://drive.google.com/file/d/1fNpuJEOPCKKznUHQud16rAkc7lrh98Ya/view?usp=sharing), unzip it and save it under `data`.



## Best Model Weights
You can download the weights of the best variant in:
[best model weights](https://drive.google.com/drive/folders/11k_BTiXD6ItjEhN4wp267HRjg1euttL7?usp=sharing)
