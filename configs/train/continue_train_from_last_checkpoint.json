{
    // Model config
    "experiment_type": "seq2seq",
    "model_name_or_path": "models/T0_3B/T0_3B_no_CoT_no_icl/checkpoint-300",
    "output_dir": "models/T0_3B/T0_3B_no_CoT_no_icl",
    "source_prefix": "Instruction: In this task, you are presented with a passage, where some parts are \"highlighted\" (namely, there are <highlight_start> and <highlight_end> tokens before and after each such span). \nYour job is to generate a summary that covers all and only the \"highlighted\" spans. \nPassage: ",
    "max_source_length": 4096,
    "max_target_length": 512,
    // "add_global_attention": true,
    // "add_global_attention_on_highlights": true,
    "overwrite_cache": true,  // add this if getting the error CUBLAS_STATUS_ALLOC_FAILED
    "resume_from_checkpoint": true, // Useful if training crashed
    // Model configs copied from the colab to finetune led https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing#scrollTo=kQOaX6eRJXkM
    "gradient_checkpointing": true, // Slower but allows more memory to be allocated, recommended by official page
    "min_length": 100,
    "length_penalty": 2.0,
    "early_stopping": true,
    "no_repeat_ngram_size": 3,
    "load_in_8bit": true,
    "device_map": "auto",
    // "fp16": true, // Recommended by docs
    "optim":"adafactor", // Lever to play with if getting OOM, was suggested in docs
    // "fp16_backend": "apex",  // Recommended by docs
    // "half_precision_backend": "amp",
    // "gradient_accumulation_steps": 2, // will accumulate gradient_accumulation_num * train_batch_size * num_gpus
    // Predict
    "predict_with_generate": "true",  
    "num_beams": 2,  // Lever to play with if getting OOM, was suggested in docs
    // Train
    "do_train": true,
    "per_device_train_batch_size": 2,
    "overwrite_output_dir": "true",
    "train_file": "data/train__highlights.csv",
    "save_total_limit": 2,  // Save only last one and best one
    "metric_for_best_model": "eval_gold_rougeL",
    "load_best_model_at_end": true,
    // Eval while training
    "do_eval": true,
    "per_device_eval_batch_size": 2,  // Sometimes can be larger than training batch size (no grad is activated)
    "validation_file": "data/dev__highlights.csv",
    "evaluation_strategy": "steps",
    "save_steps": 100,
    "eval_steps": 100,
    "logging_steps": 50,
    "num_train_epochs": 10.0,
    // Wandb
    "report_to": "wandb",
    "run_name": "T0/T0_3B_no_CoT_no_icl"
}