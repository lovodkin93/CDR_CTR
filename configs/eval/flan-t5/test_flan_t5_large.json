{
    // Model config
    "experiment_type": "seq2seq",      
    "model_name_or_path": "/home/nlp/sloboda1/controlled_reduction/CTR_instruction_finetuning/models/flan-t5-large/flan_t5_large_no_CoT_no_icl_start_with_instruction_word_max_source_length_1400/checkpoint-4100",
    "source_prefix": "Instruction: In this task, you are presented with a passage, where some parts are \"highlighted\" (namely, there are <extra_id_1> and <extra_id_2> tokens before and after each such span). \nYour job is to generate a summary that covers all and only the \"highlighted\" spans. \nPassage: ",
    "output_dir": "models/predict/test/flan-t5-large/flan_t5_large_no_CoT_no_icl_start_with_instruction_word_max_source_length_1400",
    "max_source_length": 1400,
    "max_target_length": 200,  // Lowering this will yield unreliable rouge results (based only on the limited summary)!
    "overwrite_cache": true,  // add this if getting the error CUBLAS_STATUS_ALLOC_FAILED
    "eval_with_summac": false,
    // "gradient_checkpointing": true, // Slower but allows more memory to be allocated, recommended by official page
    "min_length": 100,
    "length_penalty": 2.0,
    "early_stopping": true,
    "no_repeat_ngram_size": 3,
    "max_grad_norm": 1.0,
    // "sharded_ddp": "zero_dp_3 auto_wrap",
    // "load_in_8bit": true,
    // "device_map": "auto",
    // Train config                
    "overwrite_output_dir": true,
    "validation_file": "data/test__highlights.csv", // Necessary just not to crash run.py, doesn't do anything
    // Predict
    "do_predict": true,
    "predict_with_generate": true,
    "test_file": "data/test__highlights.csv",
    "per_device_eval_batch_size": 4,  // Sometimes can be larger than training batch size (no grad is activated)
    "num_beams": 2,  // Lever to play with if getting OOM
    // Cancel Wandb
    "report_to": "none"
    // Debug
    // "max_predict_samples": 24
}