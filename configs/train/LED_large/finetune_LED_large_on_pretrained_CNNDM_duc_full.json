{
// Model config
"experiment_type": "seq2seq",
"model_name_or_path": "models/led-large-16384/pretrain_cnndm_duc/checkpoint-14400",
"source_prefix": "",
"max_source_length": 1400,
"max_target_length": 512,
"output_dir": "models/led-large-16384/finetune_led_large_on_pretrain_cnndm_duc",
// "resume_from_checkpoint": true, // Useful if training crashed
"add_global_attention": true,
"add_global_attention_on_highlights": true,
// "gradient_checkpointing": true, // Slower but allows more memory to be allocated, recommended by official page
// "gradient_accumulation_steps":4,
"min_length": 100,
"length_penalty": 2.0,
"early_stopping": true,
"no_repeat_ngram_size": 3,
// "load_in_8bit": true,
// "device_map": "auto",
"fp16": false, // Recommended by docs
// "optim":"adafactor", // Lever to play with if getting OOM, was suggested in docs
// "fp16_backend": "apex",  // Recommended by docs
// Predict
// "do_predict",  # Loading predict dataset just loads the memory un-necessairly
"predict_with_generate": true,  
"num_beams": 2,  // Lever to play with if getting OOM, was suggested in docs
// Train              
"do_train": true,
"per_device_train_batch_size": 6,
"overwrite_output_dir": "true",
"train_file": "data/train__highlights.csv",
"save_total_limit": 2,  // Save only last one and best one
"metric_for_best_model": "eval_gold_rougeL",
"load_best_model_at_end": true,
// "lora_training": true,
"max_grad_norm": 1.0,
"sharded_ddp": "zero_dp_3 auto_wrap", // to accomodate the model across the GPUs (by sharding it and saving different layers of it in different GPUs)
// "fsdp_transformer_layer_cls_to_wrap": "T5Block", // if choosing non-T5 block, change to other model.
// "fsdp": "full_shard auto_wrap",
// "eval_accumulation_steps": 1,
// "learning_rate": 1e-9,
// "add_CoT_to_output": "True",
// "add_icl_to_input": "True",
// Eval while training
"do_eval": true,
"per_device_eval_batch_size": 6,  // Can be larger than training batch size (no grad is activated)
"validation_file": "data/dev__highlights.csv",
// "fp16_full_eval": true,
"evaluation_strategy": "steps",
"save_steps": 100,
"eval_steps": 100,
"logging_steps": 100,
"num_train_epochs": 10.0,
// Wandb
"report_to": "wandb",
"run_name": "led-large/finetune_led_large_on_pretrain_cnndm_duc"
}